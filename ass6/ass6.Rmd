---
title: "Assignment 6"
author: "Adrian Bracher (Matr. Nr. 01637180)"
date: "05.05.2021"
output:
  pdf_document: 
    toc: true 
    toc_depth: 3
    number_sections: true 
---
# Downloading Files and Using API Clients

```{r echo=FALSE}

library(dplyr)
```

## Downloading files and reading them into R
In the code below we download and read comma-separated value files (csv) and tab-separated value files (tsv) using urls.
```{r}
# Here are the URLs! As you can see they're just normal strings
csv_url <- "http://s3.amazonaws.com/assets.datacamp.com/production/course_1561/datasets/chickwts.csv"
tsv_url <- "http://s3.amazonaws.com/assets.datacamp.com/production/course_3026/datasets/tsv_data.tsv"

# Read a file in from the CSV URL and assign it to csv_data
csv_data <- read.csv(csv_url)

# Read a file in from the TSV URL and assign it to tsv_data
tsv_data <- read.delim(tsv_url)

# Examine the objects with head()
head(csv_data)
head(tsv_data)
```

## Saving raw files to disk
In this exercise we first download and save a csv (to our local directory) and then read it from the local file with read.csv. 
```{r}
# Download the file with download.file()
download.file(url = csv_url, destfile = "feed_data.csv")

# Read it in with read.csv()
csv_data <- read.csv("feed_data.csv")
```

## Saving formatted files to disk
In this exercise we learn how to modify a csv data structure and then write and read an RDS file. 
```{r}
# Add a new column: square_weight
csv_data$square_weight <- csv_data$weight^2

# Save it to disk with saveRDS()
saveRDS(object = csv_data, file = "modified_feed_data.RDS")

# Read it back in with readRDS()
modified_feed_data <- readRDS(file = "modified_feed_data.RDS")

# Examine modified_feed_data
str(modified_feed_data)
```

## Using API clients
In the code below we use the Wikipedia API (and the library pageviews, which acts as an API client) to get the pageviews for the "Hadley Wickham" article.
```{r}
# Load pageviews
library(pageviews)

# Get the pageviews for "Hadley Wickham"
hadley_pageviews <- article_pageviews(project = "en.wikipedia", "Hadley Wickham")

# Examine the resulting object
str(hadley_pageviews)
```

## Using access tokens
We use the access token stored in api_key to get information of the word usage of "vector" in the Wordnik dictionary.
```{r eval=FALSE}
# Load birdnik
library(birdnik)

# Get the word frequency for "vector", using api_key to access it
vector_frequency <- word_frequency(api_key, "vector")
```

# Using httr to interact with APIs directly
## GET requests in practice
In this exercise we learn how to call GET to request a resource from a specified URL.
```{r}
# Load the httr package
library(httr)

# Make a GET request to http://httpbin.org/get
get_result <- GET("http://httpbin.org/get")

# Print it to inspect it
print(get_result)
```

## POST requests in practice
In the code below we use the httr package to send a POST-Request with a specified HTML body.
```{r}
# Load the httr package
library(httr)

# Make a POST request to http://httpbin.org/post with the body "this is a test"
post_result <- POST("http://httpbin.org/post", body="this is a test")

# Print it to inspect it
post_result
```

## Extracting the response
First we get a response, then the actual data in the response is extracted and the result is then analyzed with str().
```{r}
url = "http://httpbin.org/get"
# Make a GET request to url and save the results
pageview_response <- GET(url)

# Call content() to retrieve the data the server sent back
pageview_data <- content(pageview_response)

# Examine the results with str()
str(pageview_data)
```

## Handling http failures
Occuring HTTP errors can be handled like below:
```{r}
fake_url <- "http://google.com/fakepagethatdoesnotexist"

# Make the GET request
request_result <- GET(fake_url)

# Check request_result
if(http_error(request_result)){
	warning("The request failed")
} else {
	content(request_result)
}
```

## Constructing queries (Part 1)
We construct a url out of multiple parts with the function paste().
```{r}
# Construct a directory-based API URL to `http://swapi.co/api`,
# looking for person `1` in `people`
directory_url <- paste("http://swapi.co/api", "people", 1, sep = "/")

# Make a GET call with it
result <- GET(directory_url)
```

## Constructing queries (Part 1)
In this exercise we use the query parameter and pass key-value pairs as list instead of creating the url explicitly.
```{r}
# Create list with nationality and country elements
query_params <- list(nationality = "americans", 
    country = "antigua")
    
# Make parameter-based call to httpbin, with query_params
parameter_response <- GET("https://httpbin.org/get", query = query_params)

# Print parameter_response
parameter_response
```

## Using user agents
User-agents are used to let the API know who it is interacting with. It should include an email-address and a url for the project the code is part of.
```{r}
# Do not change the url
url <- "https://wikimedia.org/api/rest_v1/metrics/pageviews/per-article/en.wikipedia/all-access/all-agents/Aaron_Halfaker/daily/2015100100/2015103100"

# Add the email address and the test sentence inside user_agent()
server_response <- GET(url, user_agent("my@email.address this is a test"))
```

## Rate-limiting
Another part of respectful API usage, is limiting the number of requests for a given period. In the example below we make a requests every 5 seconds.
```{r}
# Construct a vector of 2 URLs
urls <- c("http://httpbin.org/status/404", "http://httpbin.org/status/301")

for(url in urls){
    # Send a GET request to url
    result <- GET(url)
    # Delay for 5 seconds between requests
    Sys.sleep(5)
}
```

## Tying it all together
In the example below we combine previously learned knowledge about APIs.
```{r}
get_pageviews <- function(article_title){
  url <- paste(
    "https://wikimedia.org/api/rest_v1/metrics/pageviews/per-article/en.wikipedia/all-access/all-agents", 
    article_title, 
    "daily/2015100100/2015103100", 
    sep = "/"
  )   
  response <- GET(url, user_agent("my@email.com this is a test")) 
  # Is there an HTTP error?
  if(http_error(response)){ 
    # Throw an R error
    stop("the request failed") 
  }
  # Return the response's content
  content(response)
}
```

# JSON
## Parsing JSON
In the code below different ways to parse JSON data are compared.
```{r eval=FALSE}
# Get revision history for "Hadley Wickham"
resp_json <- rev_history("Hadley Wickham")

# Check http_type() of resp_json
http_type(resp_json)

# Examine returned text with content()
content(resp_json, as="text")

# Parse response with content()
content(resp_json, as="parsed")

# Parse returned text with fromJSON()
library(jsonlite)
fromJSON(content(resp_json, as="text"))
```

## Manipulating parsed JSON
JSON data is parsed into a list and can then be extracted with the rlist library.
```{r eval=FALSE}
# Load rlist
library(rlist)

# Examine output of this code
str(content(resp_json), max.level = 4)

# Store revision list
revs <- content(resp_json)$query$pages$`41916270`$revisions

# Extract the user element
user_time <- list.select(revs, user, timestamp)

# Print user_time
user_time

# Stack to turn into a data frame
list.stack(user_time)
```

## Reformatting JSON
Alternatively to rlist the dplyr package can be used to extract data from JSON.
```{r eval=FALSE}
# Load dplyr
library(dplyr)

# Pull out revision list
revs <- content(resp_json)$query$pages$`41916270`$revisions

# Extract user and timestamp
revs %>%
  bind_rows() %>%           
  select(user, timestamp)
```

## Examining XML documents
XML documents can be read with the xml2 library like below. 
```{r eval=FALSE}
# Load xml2
library(xml2)


# Get XML revision history
resp_xml <- rev_history("Hadley Wickham", format = "xml")

# Check response is XML 
http_type(resp_xml)

# Examine returned text with content()
rev_text <- content(resp_xml, as="text")
rev_text

# Turn rev_text into an XML document
rev_xml <- read_xml(rev_text)

# Examine the structure of rev_xml
xml_structure(rev_xml)
```

## Extracting XML data
XPATHs can be used to find nodes in an XML document. /node_name find nodes at a certain level with a certain name and //node_name specifies nodes at any level below the current level. 
```{r eval=FALSE}
# Find all nodes using XPATH "/api/query/pages/page/revisions/rev"
xml_find_all(rev_xml, "/api/query/pages/page/revisions/rev")

# Find all rev nodes anywhere in document
rev_nodes <- xml_find_all(rev_xml, "//rev")

# Use xml_text() to get text from rev_nodes
xml_text(rev_nodes)
```

## Extracting XML attributes
Selecting XML attributes can be done very similar to nodes with XPATHs, but the function xml_attr and xml_attrs are used instead of xml_find_all.
```{r eval=FALSE}
# All rev nodes
rev_nodes <- xml_find_all(rev_xml, "//rev")

# The first rev node
first_rev_node <- xml_find_first(rev_xml, "//rev")

# Find all attributes with xml_attrs()
xml_attrs(first_rev_node)

# Find user attribute with xml_attr()
xml_attr(first_rev_node, "user")

# Find user attribute for all rev nodes
xml_attr(rev_nodes, "user")

# Find anon attribute for all rev nodes
xml_attr(rev_nodes, "anon")
```

## Wrapup: returning nice API output
Everything that we learned in this chapter is used in this exercise to use an API and extract useful information.
```{r eval=FALSE}
get_revision_history <- function(article_title){
  # Get raw revision response
  rev_resp <- rev_history(article_title, format = "xml")
  
  # Turn the content() of rev_resp into XML
  rev_xml <- read_xml(content(rev_resp, "text"))
  
  # Find revision nodes
  rev_nodes <- xml_find_all(rev_xml, "//rev")

  # Parse out usernames
  user <- xml_attr(rev_nodes, "user")
  
  # Parse out timestamps
  timestamp <- readr::parse_datetime(xml_attr(rev_nodes, "timestamp"))
  
  # Parse out content
  content <- xml_text(rev_nodes)
  
  # Return data frame 
  data.frame(user = user,
    timestamp = timestamp,
    content = substr(content, 1, 40))
}

# Call function for "Hadley Wickham"
get_revision_history(article_title="Hadley Wickham")
```
# Web scraping with XPATHs
## Reading HTML
In this exercise we use rvest to read an html file via url.
```{r eval=FALSE}
# Load rvest
library(rvest)

# Hadley Wickham's Wikipedia page
test_url <- "https://en.wikipedia.org/wiki/Hadley_Wickham"

# Read the URL stored as "test_url" with read_html()
test_xml <- read_html(test_url)

# Print test_xml
test_xml
```

## Extracting nodes by XPATH
Here we use XPATH to select individual, identifiable nodes.
```{r eval=FALSE}
# Use html_node() to grab the node with the XPATH stored as `test_node_xpath`
node = html_node(x = test_xml, xpath = test_node_xpath)

# Print the first element of the result
head(node)
```

## Extracting names
The function html_name returns the name of the first element of the html argument, in this case "table".
```{r eval=FALSE}
# Extract the name of table_element
element_name <- html_name(table_element)

# Print the name
print(element_name)
```

## Extracting values
Values can be extracted in a similar fashion with html_text().
```{r eval=FALSE}
# Extract the element of table_element referred to by second_xpath_val and store it as page_name
page_name <- html_node(x = table_element, xpath = second_xpath_val)

# Extract the text from page_name
page_title <- html_text(page_name)

# Print page_title
page_title
```

## Extracting tables
The function html_table is used to convert a node element containing a table to a data frame 
```{r eval=FALSE}
# Turn table_element into a data frame and assign it to wiki_table
wiki_table <- html_table(table_element)

# Print wiki_table
wiki_table
```

## Cleaning a data frame
In this exercise we learn how to rename columns and remove empty rows from our data.
```{r eval=FALSE}
# Rename the columns of wiki_table
colnames(wiki_table) <- c("key", "value")

# Remove the empty row from wiki_table
cleaned_table <- subset(wiki_table, ! key == "")

# Print cleaned_table
cleaned_table
```

# CSS Web Scraping and Final Case Study
## Using CSS to scrape nodes
In this exercise we learn how to use CSS selectors to select nodes.
```{r}
library(rvest)
library(xml2)
test_url <- "https://en.wikipedia.org/wiki/Hadley_Wickham"

# Read the URL stored as "test_url" with read_html()
test_xml <- read_html(test_url)

# Select the table elements
html_nodes(test_xml, css = "table")

# Select elements with class = "infobox"
html_nodes(test_xml, css = ".infobox")

# Select elements with id = "firstHeading"
html_nodes(test_xml, css = '#firstHeading')
```

## Scraping names
The css selector starting with "." selects classes, "#" selects IDs.
```{r}
# Extract element with class infobox
infobox_element <- html_nodes(test_xml, css = ".infobox")

# Get tag name of infobox_element
element_name <- html_name(infobox_element)

# Print element_name
element_name
```

## Scraping text
The function html_name can be used to get the name of the specified node element.
```{r}
# Extract element with class infobox
infobox_element <- html_nodes(test_xml, css = ".infobox")

# Get tag name of infobox_element
element_name <- html_name(infobox_element)

# Print element_name
element_name
```

## API calls
In this exercise we make an API call to wikipedia like we did in the first chapter.
```{r}
# Load httr
library(httr)

# The API url
base_url <- "https://en.wikipedia.org/w/api.php"

# Set query parameters
query_params <- list(action = "parse", 
  page = "Hadley Wickham", 
  format = "xml")

# Get data from API
resp <- GET(url = base_url, query = query_params)
    
# Parse response
resp_xml <- content(resp)
```

## Extracting information
In the code below we extract different information from an html.
```{r}
# Load rvest
library(rvest)

# Read page contents as HTML
page_html <- read_html(xml_text(resp_xml))

# Extract infobox element
infobox_element <- html_node(page_html, css=".infobox")

# Extract page name element from infobox
page_name <- html_node(infobox_element, css=".fn")

# Extract page name as text
page_title <- html_text(infobox_element)
```

## Normalising information
In the code below we parse the infobox into a data frame.
```{r}
# Your code from earlier exercises
wiki_table <- html_table(infobox_element)
colnames(wiki_table) <- c("key", "value")
cleaned_table <- subset(wiki_table, !key == "")

# Create a dataframe for full name
name_df <- data.frame(key = "Full name", value = page_title)

# Combine name_df with cleaned_table
wiki_table2 <- rbind(name_df, cleaned_table)

# Print wiki_table
wiki_table2
```

## Reproducibility
In this exercise we fix a function and test it with different inputs.
```{r}
library(httr)
library(rvest)
library(xml2)

get_infobox <- function(title){
  base_url <- "https://en.wikipedia.org/w/api.php"
  
  # Change "Hadley Wickham" to title
  query_params <- list(action = "parse", 
    page = "Hadley Wickham", 
    format = "xml")
  
  resp <- GET(url = base_url, query = query_params)
  resp_xml <- content(resp)
  
  page_html <- read_html(xml_text(resp_xml))
  infobox_element <- html_node(x = page_html, css =".infobox")
  page_name <- html_node(x = infobox_element, css = ".fn")
  page_title <- html_text(page_name)
  
  wiki_table <- html_table(infobox_element)
  colnames(wiki_table) <- c("key", "value")
  cleaned_table <- subset(wiki_table, !wiki_table$key == "")
  name_df <- data.frame(key = "Full name", value = page_title)
  wiki_table <- rbind(name_df, cleaned_table)
  
  wiki_table
}

# Test get_infobox with "Hadley Wickham"
get_infobox(title = "Hadley Wickham")

# Try get_infobox with "Ross Ihaka"
get_infobox(title = "Ross Ihaka")

# Try get_infobox with "Grace Hopper"
get_infobox(title = "Grace Hopper")
```
